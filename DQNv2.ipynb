{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31197381",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eba1cf-3709-4a56-a1c2-9a6fcefc1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9583bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqnv2 import DQNAgentV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf58c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 3_000\n",
    "start_epsilon = 0.9\n",
    "final_epsilon = 0.05\n",
    "epsilon_decay = 0.95\n",
    "# reduce the exploration over time\n",
    "batch_size = 64\n",
    "discount_factor = 0.99\n",
    "replay_size = 10_000\n",
    "logging_interval = 10\n",
    "hidden_size=64\n",
    "dropout_rate=0.0\n",
    "weight_decay=1e-4\n",
    "target_network = False\n",
    "target_network_update = int(1e4)\n",
    "alpha = 0\n",
    "seed=42\n",
    "np.random.seed(seed)    \n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "amsgrad = False\n",
    "reward_hidden_size=64\n",
    "reward_factor=1/5\n",
    "running_window = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3c32d8-7582-4593-b9e8-d1e555578849",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgentV2(\n",
    "    learning_rate=learning_rate,\n",
    "    state_size=2,\n",
    "    action_size=3,\n",
    "    discount_factor=discount_factor,\n",
    "    final_epsilon=final_epsilon,\n",
    "    hidden_size=hidden_size,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    replay_size=replay_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    target_network=target_network,\n",
    "    weight_decay=weight_decay,\n",
    "    target_network_update=target_network_update,\n",
    "    alpha=alpha,\n",
    "    amsgrad=amsgrad,\n",
    "    reward_hidden_size=reward_hidden_size,\n",
    "    reward_factor=reward_factor,\n",
    "    running_window=running_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c358b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnreateguir\u001b[0m (\u001b[33mreategui\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nicolasrr/Desktop/ANN-RL/wandb/run-20240518_223652-82u8b7zn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reategui/ANN/runs/82u8b7zn' target=\"_blank\">DQNv2</a></strong> to <a href='https://wandb.ai/reategui/ANN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reategui/ANN' target=\"_blank\">https://wandb.ai/reategui/ANN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reategui/ANN/runs/82u8b7zn' target=\"_blank\">https://wandb.ai/reategui/ANN/runs/82u8b7zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='ANN', config={\"learning_rate\": learning_rate, \"n_episodes\": n_episodes, \"start_epsilon\": start_epsilon, \"final_epsilon\": final_epsilon, \"epsilon_decay\": epsilon_decay, \"batch_size\": batch_size, \"discount_factor\": discount_factor, \"replay_size\": replay_size, \"hidden_size\": hidden_size, \"dropout_rate\": dropout_rate, \"weight_decay\":weight_decay, \"target_network\":target_network, \"alpha\":alpha,\"target_network_update\":target_network_update, \"reward_factor\":reward_factor, \"reward_hidden_size\":reward_hidden_size, \"amsgrad\":amsgrad}, name='DQNv2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36016c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 0/3000:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 585/3000 [07:22<30:26,  1.32it/s]<38:48,  1.04it/s, episode_RND_reward=3.94, episode_steps=200, epsilon=0.05, intrinsic_loss=1.14e-8, reward=-196, target_count=107001, train_loss=0.0534] \n",
      "Episode 585/3000:  20%|█▉        | 585/3000 [07:22<30:26,  1.32it/s, episode_RND_reward=3.94, episode_steps=200, epsilon=0.05, intrinsic_loss=1.14e-8, reward=-196, target_count=107001, train_loss=0.0534]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[1;32m     28\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m loss, target_count, RND,intrinsic_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m RND\n",
      "File \u001b[0;32m~/Desktop/ANN-RL/dqnv2.py:78\u001b[0m, in \u001b[0;36mDQNAgentV2.update\u001b[0;34m(self, obs, action, reward, next_obs, target_count, batch_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m predicted_intrinsic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(torch\u001b[38;5;241m.\u001b[39mtensor(next_obs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     77\u001b[0m target_intrinsic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_predictor(torch\u001b[38;5;241m.\u001b[39mtensor(next_obs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m intrinsic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion_intrinsic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_intrinsic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_intrinsic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#################\u001b[39;00m\n\u001b[1;32m     82\u001b[0m sample_replay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha)\n",
      "File \u001b[0;32m~/miniconda3/envs/ann/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ann/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ann/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ann/lib/python3.10/site-packages/torch/nn/functional.py:3366\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "with tqdm(total=n_episodes, desc=f\"Episode 0/{n_episodes}\") as pbar:\n",
    "    losses = []\n",
    "    rewards = []\n",
    "    target_count = 0\n",
    "    finished = []\n",
    "    episode_steps = []\n",
    "    empty = True\n",
    "    intrinsic_losses = []\n",
    "    RND_rewards = []\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        qnetwork_loss = 0\n",
    "        intrinsic_loss = 0\n",
    "        episode_RND_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(obs, env)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update if the environment is done and the current obs\n",
    "            done = terminated or truncated\n",
    "            if terminated:\n",
    "                next_obs = (None, None)\n",
    "\n",
    "            loss, target_count, RND,intrinsic_loss = agent.update(obs, action, reward, next_obs, batch_size=batch_size, target_count=target_count)\n",
    "            if loss is not None:\n",
    "                episode_reward += reward + RND\n",
    "                qnetwork_loss+=loss\n",
    "                intrinsic_loss+=intrinsic_loss\n",
    "                episode_RND_reward+=RND\n",
    "\n",
    "            obs = next_obs\n",
    "            t+=1\n",
    "        pbar.set_description(f\"Episode {episode + 1}/{n_episodes}\")\n",
    "        pbar.set_postfix(train_loss=qnetwork_loss, epsilon=agent.epsilon, target_count=target_count, intrinsic_loss=intrinsic_loss, episode_steps=t, reward=episode_reward, episode_RND_reward=episode_RND_reward)\n",
    "        pbar.update(1)\n",
    "        pbar.refresh() \n",
    "        if not empty:\n",
    "            finished.append(terminated)\n",
    "            episode_steps.append(t)\n",
    "            rewards.append(episode_reward)\n",
    "            losses.append(qnetwork_loss)\n",
    "            intrinsic_losses.append(intrinsic_loss)\n",
    "            agent.decay_epsilon()\n",
    "            RND_rewards.append(episode_RND_reward)\n",
    "            if episode % logging_interval == 0 :\n",
    "                wandb.log({\"train_loss\": np.mean(losses), \"predicted_intrinsic_losses\":np.mean(intrinsic_losses), \"epsilon\": agent.epsilon, \"episode_steps\": np.mean(episode_steps), \"finished\": np.sum(finished), \"mean_reward\": np.mean(rewards), \"episode_RND_reward\": np.mean(RND_rewards)})\n",
    "                losses = []\n",
    "                rewards = []\n",
    "                finished = []\n",
    "                episode_steps = []\n",
    "                RND_rewards = []\n",
    "                intrinsic_losses = []\n",
    "        if loss is not None:\n",
    "            empty = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
